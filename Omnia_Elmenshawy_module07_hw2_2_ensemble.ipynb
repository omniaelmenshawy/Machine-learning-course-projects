{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvT_D8NibK47"
      },
      "source": [
        "# HOMEWORK 2.2: Ensemble Learning\n",
        "\n",
        "# Omnia Elmenshawy -  2000007 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VwurtX6bK49"
      },
      "source": [
        "# Description of HW2.2\n",
        "\n",
        "The aim of this assignment is to use Ensemble Learning to solve a problem. In this way, it is aimed to understand the benefits of Ensemble Learning and to teach the usage details of different ensemble approaches with the help of ScikitLearn library. \n",
        "\n",
        "The following methods will be implemented within the scope of this assignment. These are:\n",
        "- Voting Classifier (hard & soft voting) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%% \n"
        },
        "id": "mrxBWUHzbK4-"
      },
      "outputs": [],
      "source": [
        "# Common imports\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%% \n"
        },
        "id": "RpNfEDvobK4_"
      },
      "outputs": [],
      "source": [
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "jNfxO51qbK5A"
      },
      "outputs": [],
      "source": [
        "# generate moon dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC7J2lLybK5A"
      },
      "source": [
        "# TODO 2.2.1 Voting classifier in Scikit-Learn\n",
        "- Train voting classifiers in Scikit-Learn, composed of at least three diverse classifiers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v456OW2bK5B",
        "outputId": "0a7e3fc1-86f9-47af-b7ff-23ccd4b0af8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hard voting classifiers result:  GradientBoostingClassifier 0.872\n",
            "Hard voting classifiers result:  RandomForestClassifier 0.896\n",
            "Hard voting classifiers result:  LogisticRegression 0.864\n",
            "Hard voting classifiers result:  VotingClassifier 0.904\n",
            " \n",
            "Soft voting classifiers result:  GradientBoostingClassifier 0.872\n",
            "Soft voting classifiers result:  RandomForestClassifier 0.896\n",
            "Soft voting classifiers result:  LogisticRegression 0.864\n",
            "Soft voting classifiers result:  VotingClassifier 0.896\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "clf1 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,  max_depth=1, random_state=0)\n",
        "clf2 = RandomForestClassifier(random_state=42)\n",
        "clf3 = LogisticRegression(random_state=42)\n",
        "\n",
        "#define your hard voting classifier\n",
        "voting_clf_hard = VotingClassifier(\n",
        "    estimators=[('GB', clf1), ('rf', clf2), ('lr', clf3)],\n",
        "    voting='hard')\n",
        "\n",
        "#Extra: defining my soft classifier\n",
        "\n",
        "voting_clf_soft = VotingClassifier(\n",
        "    estimators=[('GB', clf1), ('rf', clf2), ('lr', clf3)],\n",
        "    voting='soft')\n",
        "\n",
        "#train your voting classifier using X_train, y_train\n",
        "voting_clf_hard.fit(X_train, y_train)\n",
        "voting_clf_soft.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "#write obtained individual classifiers. Compare them with \"Voting Classifiers (hard and soft)\" for \"X_test/y_test\" data\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "for model in (clf1, clf2, clf3, voting_clf_hard):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"Hard voting classifiers result: \", model.__class__.__name__, accuracy_score(y_test, y_pred))\n",
        "    \n",
        "print(\" \")\n",
        "\n",
        "for model in (clf1, clf2, clf3, voting_clf_soft):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"Soft voting classifiers result: \", model.__class__.__name__, accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Based on the previous result, the hard classifier has a better accuracy."
      ],
      "metadata": {
        "id": "xR_--rbbftWt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QasqdXabK5B"
      },
      "source": [
        "# TODO 2.2.2 Bagging/Pasting\n",
        "- One way to get a diverse set of classifiers is to use very different training algorithms\n",
        "- Another approach is to use the same training algorithm for every predictor, but to train them on different random subsets of the training set\n",
        "    - When sampling is performed with replacement, this method is called ***bagging*** \n",
        "        - short for ***bootstrap aggregating***\n",
        "    - When sampling is performed without replacement, it is called pasting\n",
        "- Both bagging and pasting allow training instances to be sampled several times across multiple predictors\n",
        "- Only bagging allows training instances to be sampled several times for the same predictor\n",
        "- Predictors can all be trained in parallel, via different CPU cores or even different servers.\n",
        "- Similarly, predictions can be made in parallel.\n",
        "- This is one of the reasons why bagging and pasting are such popular methods: they scale very well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t60s98SQbK5C",
        "outputId": "79086f6c-e84a-4512-db24-5e58898d80a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy score of bagging classifier is:  0.904\n",
            " \n",
            "accuracy score of pasting classifier is:  0.92\n"
          ]
        }
      ],
      "source": [
        "# Scikit-Learn offers a simple API for both bagging and pasting with the BaggingClassifier class \n",
        "#  \n",
        "#   (this is an example of bagging, but if you want to use pasting instead, just set bootstrap=False). \n",
        "# The n_jobs parameter tells Scikit-Learn the number of CPU cores to use for training and predictions \n",
        "#   (â€“1 tells Scikit-Learn to use all available cores):\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "# Write Bagging Classifier and train it using ensemble of 500 Decision Tree classifiers, \n",
        "#  each trained on 100 training instances randomly sampled from the training set with replacement \n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
        "    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n",
        "\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "# bagging accuracy score\n",
        "# Using X_test dataset calculate/print Bagging Accuracy Score (using accuracy_score metric)\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"accuracy score of bagging classifier is: \", accuracy_score(y_test, y_pred))\n",
        "\n",
        "print(\" \")\n",
        "\n",
        "#Pasting classifier:\n",
        "paste_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
        "    max_samples=100, bootstrap=False, n_jobs=-1, random_state=42)\n",
        "\n",
        "paste_clf.fit(X_train, y_train)\n",
        "y_pred = paste_clf.predict(X_test)\n",
        "\n",
        "print(\"accuracy score of pasting classifier is: \", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ADQiF5WbK5D",
        "outputId": "af203640-9c5a-47db-c4c9-7e34b1ec2ffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.856\n"
          ]
        }
      ],
      "source": [
        "# Without bagging accuracy score\n",
        "tree_clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "# Calculate \"Tree Classifier\" accuracy score\n",
        "# Using X_test dataset calculate/print Tree Classifier Accuracy Score (using accuracy_score metric)\n",
        "\n",
        "tree_clf.fit(X_train, y_train)\n",
        "y_pred_tree = tree_clf.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred_tree))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Pasting Classifier is has higher accuracy than the bagging and decidion trees classifier."
      ],
      "metadata": {
        "id": "Pq_haLVdi0ms"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flRAGtAabK5D"
      },
      "source": [
        "\n",
        "# TODO 2.2.2 Out-of-Bag evaluation\n",
        "- With bagging, some instances may be sampled several times for any given predictor, \n",
        " while others may not be sampled at all. \n",
        "- By default a BaggingClassifier samples m training instances with replacement (bootstrap=True), \n",
        " where m is the size of the training set\n",
        "- Only about 60% of the training instances are sampled on average for each predictor\n",
        "- The remaining 40% of the training instances that are not sampled are called out-of-bag (oob) instances\n",
        "- Since a predictor never sees the oob instances during training, it can be evaluated on these instances, \n",
        "without the need for a separate validation set or cross-validation\n",
        "- In Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier\n",
        " to request an automatic oob evaluation after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Drv--cLRbK5E",
        "outputId": "f26d8e31-c313-4d5a-f504-4719f5f09d48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB score is:  0.9253333333333333\n",
            "accuracy score of OBB  is:  0.904\n"
          ]
        }
      ],
      "source": [
        "#write your out-of-bag (oob) classifier and train it. \n",
        " \n",
        "oob_bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
        "    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42, oob_score=True)\n",
        "\n",
        "oob_bag_clf.fit(X_train, y_train)\n",
        "y_pred_oob = oob_bag_clf.predict(X_test)\n",
        "\n",
        "\n",
        "# According to this oob evaluation print your oob score for test dataset \n",
        "oob_bag_clf.oob_score_\n",
        "print(\"OOB score is: \", oob_bag_clf.oob_score_)\n",
        "\n",
        "\n",
        "\n",
        "# Calculate oob bagging accuracy score\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"accuracy score of OBB  is: \", accuracy_score(y_test, y_pred_oob))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "H2WLM3p-bK5E"
      },
      "source": [
        "# TODO 2.2.4 Random Forests\n",
        "- Random Forest is an ensemble of Decision Trees\n",
        "- Generally trained via the bagging method typically with max_samples set to the size of the training set\n",
        "- Instead of building a BaggingClassifier and passing it a DecisionTreeClassifier, \n",
        "you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees\n",
        "\n",
        "- Train a Random Forest classifier with 500 trees (each limited to maximum 16 nodes), using all available CPU cores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZdlyqCpbK5F",
        "outputId": "b92963e9-989e-4567-f16c-6f486ba00340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy score of Random  classifier is:  0.912\n",
            " \n",
            "accuracy score of Bag Random  classifier is:  0.912\n"
          ]
        }
      ],
      "source": [
        "# RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "# Write your RandomForestClassifier classifier using sklearn RandomForestClassifier class and train it. \n",
        "rnd_clf =  RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n",
        "\n",
        "\n",
        "# Calculate/print the prediction accuracy of the rnd_clf for Test Data\n",
        "\n",
        "\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rnd_clf.predict(X_test)\n",
        "print(\"accuracy score of Random  classifier is: \", accuracy_score(y_test, y_pred_rf))\n",
        "\n",
        "print(\" \")\n",
        "\n",
        "# Write your RandomForestClassifier classifier using BaggingClassifier equivalent and train it.(estimator=500, leafnode=16) \n",
        "bag_rnd_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(max_features = \"sqrt\", max_leaf_nodes=16),n_estimators=500,\n",
        "    n_jobs=-1, random_state=42, bootstrap=True)\n",
        "\n",
        "# Calculate/print the prediction accuracy of the bag_rnd_clf for Test Data\n",
        "\n",
        "bag_rnd_clf.fit(X_train, y_train)\n",
        "y_pred_rf_bag = bag_rnd_clf.predict(X_test)\n",
        "print(\"accuracy score of Bag Random  classifier is: \", accuracy_score(y_test, y_pred_rf_bag))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdxJ8SX2bK5G"
      },
      "source": [
        "# TODO 2.2.5 Write Your Novel Ensemble Model \n",
        "- Write your customized Ensemble Classifier written by yourself \n",
        "- Try to get the highest score for the same dataset\n",
        "- There no limit. There is no specific constraints. Note that the classifier you write should be an Ensemble Classifier. \n",
        "\n",
        "Note: Students with the highest score on the assignment will be awarded an additional 10 points as an assignment score. All submitted scores will be ranked in descending order and the top 5 students will be awarded an additional +10 points for WH2.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb1ZM5VUbK5G",
        "outputId": "e45d3788-56e4-4812-bcd0-6165b4e4a89c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My Ensemble classifiers result:  GradientBoostingClassifier 0.92\n",
            "My Ensemble classifiers result:  KNeighborsClassifier 0.912\n",
            "My Ensemble classifiers result:  DecisionTreeClassifier 0.896\n",
            "My Ensemble classifiers result:  RandomForestClassifier 0.912\n",
            "My Ensemble classifiers result:  AdaBoostClassifier 0.904\n",
            "My Ensemble classifiers result:  VotingClassifier 0.928\n",
            " \n"
          ]
        }
      ],
      "source": [
        "# Write your Ensemble Classifier and train it. \n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "clf1 = GradientBoostingClassifier(max_depth =  3, n_estimators=15, random_state = 40)\n",
        "clf2 = KNeighborsClassifier(n_neighbors = 5)\n",
        "clf3 = DecisionTreeClassifier(random_state = 40, max_depth = 3)\n",
        "clf4 = RandomForestClassifier(n_estimators=30,  max_depth =  5,  random_state = 42)\n",
        "clf5 = AdaBoostClassifier()\n",
        "\n",
        "my_ensemble_clf =  VotingClassifier(\n",
        "    estimators=[ ('gb', clf1), ('knn', clf2), ('dt', clf3), ('rf', clf4), ('ada', clf5)],\n",
        "    voting='hard')\n",
        "\n",
        "my_ensemble_clf.fit(X_train, y_train)\n",
        "\n",
        "# Calculate accuracy score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "for model in ( clf1, clf2, clf3, clf4,clf5, my_ensemble_clf):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"My Ensemble classifiers result: \", model.__class__.__name__, accuracy_score(y_test, y_pred))\n",
        "print(\" \")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My Ensemble classifier consists of 6 classifiers with teh hard voting, and its accuracy score is 92.8%"
      ],
      "metadata": {
        "id": "hVZkP-3mZHis"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "54tqYkcA7A6O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}